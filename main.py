#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
LoterIA - Sistema de Predi√ß√£o de Loteria com Intelig√™ncia Artificial
==================================================================

M√≥dulo principal do sistema LoterIA para predi√ß√£o de resultados de loteria
usando t√©cnicas de Machine Learning e Deep Learning.

Autor: Sistema LoterIA
Data: 2025-05-31
"""

# Importa√ß√µes principais
from pickle import NONE
import pandas as pd
import numpy as np
import pyodbc
import sqlite3
import time
import itertools
from sqlalchemy import create_engine
from tqdm import tqdm
from datetime import datetime
from itertools import combinations
from sklearn.preprocessing import StandardScaler
import os
import sys

# Configurar TensorFlow
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # Reduzir logs do TensorFlow
import tensorflow as tf

# Configura√ß√£o global
class LoterIAConfig:
    """Configura√ß√µes globais do sistema LoterIA"""
    
    # Configura√ß√µes do banco de dados
    DB_TYPE = "sqlserver"  # sqlite ou sqlserver
    SQLITE_DB_PATH = "data/loteria.db"
    
    # Configura√ß√µes SQL Server (dados reais)
    SQL_SERVER = "DESKTOP-K6JPBDS"
    SQL_DATABASE = "Lotofacil"
    SQL_DRIVER = "ODBC Driver 17 for SQL Server"
    
    # Configura√ß√µes do modelo
    MODEL_SAVE_PATH = "models/"
    DATA_PATH = "data/"
    RESULTS_PATH = "results/"
    
    # Configura√ß√µes de predi√ß√£o
    DEFAULT_NUMBERS_TO_PREDICT = 15  # Para LOTOFACIL
    DEFAULT_RANGE = (1, 25)  # Range de n√∫meros da LOTOFACIL
    
    @classmethod
    def ensure_directories(cls):
        """Cria diret√≥rios necess√°rios se n√£o existirem"""
        directories = [cls.MODEL_SAVE_PATH, cls.DATA_PATH, cls.RESULTS_PATH]
        for directory in directories:
            os.makedirs(directory, exist_ok=True)
            print(f"üìÅ Diret√≥rio criado/verificado: {directory}")

class DatabaseManager:
    """Gerenciador de conex√µes com banco de dados"""
    
    def __init__(self, config: LoterIAConfig):
        self.config = config
        self.connection = None
        
    def connect(self):
        """Estabelece conex√£o com o banco de dados"""
        try:
            if self.config.DB_TYPE.lower() == "sqlite":
                self.connection = sqlite3.connect(self.config.SQLITE_DB_PATH)
                print("‚úÖ Conectado ao SQLite")
            elif self.config.DB_TYPE.lower() == "sqlserver":
                conn_str = f"DRIVER={{{self.config.SQL_DRIVER}}};SERVER={self.config.SQL_SERVER};DATABASE={self.config.SQL_DATABASE};Trusted_Connection=yes;"
                self.connection = pyodbc.connect(conn_str)
                print("‚úÖ Conectado ao SQL Server")
            else:
                raise ValueError(f"Tipo de banco n√£o suportado: {self.config.DB_TYPE}")
                
        except Exception as e:
            print(f"‚ùå Erro ao conectar ao banco: {e}")
            raise
            
    def disconnect(self):
        """Fecha conex√£o com o banco"""
        if self.connection:
            self.connection.close()
            print("üîå Conex√£o fechada")
            
    def execute_query(self, query: str, params=None):
        """Executa uma query e retorna os resultados"""
        try:
            if not self.connection:
                self.connect()
                
            if params:
                cursor = self.connection.execute(query, params)
            else:
                cursor = self.connection.execute(query)
                
            return cursor.fetchall()
            
        except Exception as e:
            print(f"‚ùå Erro ao executar query: {e}")
            raise

class DataProcessor:
    """Processador de dados para an√°lise e prepara√ß√£o"""
      def __init__(self, db_manager: DatabaseManager):
        self.db_manager = db_manager
        
    def load_historical_data(self) -> pd.DataFrame:
        """Carrega dados hist√≥ricos de resultados da tabela Resultados_INT"""
        print("üìä Carregando dados hist√≥ricos do SQL Server...")
        
        try:
            # Query para carregar dados da tabela real com TODAS as features dispon√≠veis
            query = """
            SELECT TOP 2000
                Concurso,
                Data_Sorteio as DataSorteio,
                N1, N2, N3, N4, N5, N6, N7, N8, N9, N10,
                N11, N12, N13, N14, N15,
                QtdePrimos,
                QtdeFibonacci,
                QtdeImpares,
                SomaTotal,
                Quintil1,
                Quintil2,
                Quintil3,
                Quintil4,
                Quintil5,
                QtdeGaps,
                QtdeRepetidos,
                SEQ,
                DistanciaExtremos,
                ParesSequencia,
                QtdeMultiplos3,
                ParesSaltados,
                Faixa_Baixa,
                Faixa_Media,
                Faixa_Alta,
                RepetidosMesmaPosicao
            FROM Resultados_INT
            ORDER BY Concurso DESC
            """
            
            # Conectar e executar query
            self.db_manager.connect()
            
            # Usar pandas para carregar diretamente do SQL Server
            if self.db_manager.config.DB_TYPE.lower() == "sqlserver":
                import urllib.parse
                
                # Criar connection string para SQLAlchemy
                params = urllib.parse.quote_plus(
                    f"DRIVER={{{self.db_manager.config.SQL_DRIVER}}};"
                    f"SERVER={self.db_manager.config.SQL_SERVER};"
                    f"DATABASE={self.db_manager.config.SQL_DATABASE};"
                    f"Trusted_Connection=yes;"
                )
                engine = create_engine(f"mssql+pyodbc:///?odbc_connect={params}")
                
                # Carregar dados com pandas
                df = pd.read_sql(query, engine)
                
                # Converter DataSorteio para datetime
                df['DataSorteio'] = pd.to_datetime(df['DataSorteio'])
                
                print(f"‚úÖ {len(df)} registros carregados da tabela Resultados_INT")
                print(f"üìÖ Per√≠odo: {df['DataSorteio'].min()} at√© {df['DataSorteio'].max()}")
                print(f"üé≤ Concursos: {df['Concurso'].min()} at√© {df['Concurso'].max()}")
                
                return df
            else:                # Fallback para SQLite
                results = self.db_manager.execute_query(query)
                
                if not results:
                    print("‚ö†Ô∏è Nenhum dado hist√≥rico encontrado")
                    return self._create_sample_data()
                
                # Criar DataFrame com as colunas corretas
                columns = ['Concurso', 'DataSorteio'] + [f'Num{i}' for i in range(1, 16)]
                df = pd.DataFrame(results, columns=columns[:len(results[0])])
                
                print(f"‚úÖ {len(df)} registros carregados")
                return df
            
        except Exception as e:
            print(f"‚ùå Erro ao carregar dados do SQL Server: {e}")
            print("üîÑ Tentando usar dados de exemplo...")
            return self._create_sample_data()
    
    def _create_sample_data(self) -> pd.DataFrame:
        """Cria dados de exemplo para desenvolvimento"""
        print("üîÑ Criando dados de exemplo...")
        
        # Gerar 100 jogos de exemplo
        sample_data = []
        base_date = datetime(2020, 1, 1)
        
        for i in range(100):
            # Gerar 15 n√∫meros √∫nicos entre 1 e 25
            numbers = sorted(np.random.choice(range(1, 26), 15, replace=False))
            
            game_data = {
                'Concurso': i + 1,
                'DataSorteio': base_date + pd.Timedelta(days=i*3),
            }
              # Adicionar n√∫meros no formato correto (N1 a N15)
            for j, num in enumerate(numbers, 1):
                game_data[f'N{j}'] = num                  # Adicionar algumas features b√°sicas calculadas
            game_data['QtdePrimos'] = sum(1 for n in numbers if n in [2,3,5,7,11,13,17,19,23])
            game_data['QtdeFibonacci'] = sum(1 for n in numbers if n in [1,2,3,5,8,13,21])
            game_data['QtdeImpares'] = sum(1 for n in numbers if n % 2 == 1)
            game_data['SomaTotal'] = sum(numbers)
            game_data['Quintil1'] = sum(1 for n in numbers if 1 <= n <= 5)
            game_data['Quintil2'] = sum(1 for n in numbers if 6 <= n <= 10)
            game_data['Quintil3'] = sum(1 for n in numbers if 11 <= n <= 15)
            game_data['Quintil4'] = sum(1 for n in numbers if 16 <= n <= 20)
            game_data['Quintil5'] = sum(1 for n in numbers if 21 <= n <= 25)
                
            sample_data.append(game_data)
        
        df = pd.DataFrame(sample_data)
        print(f"‚úÖ {len(df)} jogos de exemplo criados")
        return df
    
    def prepare_training_data(self, df: pd.DataFrame) -> tuple:
        """Prepara dados para treinamento do modelo com features avan√ßadas"""
        print("üîß Preparando dados para treinamento...")
          # Identificar colunas de n√∫meros (N1 a N15)
        number_columns = [f'N{i}' for i in range(1, 16)]
        
        # Verificar se as colunas existem
        available_number_cols = [col for col in number_columns if col in df.columns]
        
        if not available_number_cols:
            # Fallback para formato antigo (Num1 a Num15)
            number_columns = [f'Num{i}' for i in range(1, 16)]
            available_number_cols = [col for col in number_columns if col in df.columns]
            
            if not available_number_cols:
                # Fallback para formato mais antigo
                number_columns = [col for col in df.columns if col.startswith('num_')]
                if not number_columns:
                    raise ValueError("Nenhuma coluna de n√∫meros encontrada")
                available_number_cols = number_columns
        
        print(f"üìä Usando colunas de n√∫meros: {available_number_cols}")
        
        # Criar matriz de n√∫meros base
        X_numbers = df[available_number_cols].values
          # Adicionar features adicionais COMPLETAS se dispon√≠veis
        feature_columns = [
            # Features estat√≠sticas b√°sicas
            'QtdePrimos', 'QtdeFibonacci', 'QtdeImpares', 'SomaTotal',
            
            # Distribui√ß√£o por quintis (muito importante!)
            'Quintil1', 'Quintil2', 'Quintil3', 'Quintil4', 'Quintil5',
            
            # Padr√µes de gaps e repeti√ß√µes
            'QtdeGaps', 'QtdeRepetidos', 'SEQ', 'DistanciaExtremos',
            
            # An√°lise de sequ√™ncias
            'ParesSequencia', 'QtdeMultiplos3', 'ParesSaltados',
            
            # An√°lise espacial/faixas
            'Faixa_Baixa', 'Faixa_Media', 'Faixa_Alta',
            'RepetidosMesmaPosicao',
            
            # Features antigas (fallback)
            'QtdePares', 'Media', 'Amplitude', 'DistanciaMedia', 'QtdeSequencia',
            'Col1', 'Col2', 'Col3', 'Col4', 'Col5',
            'Linha1', 'Linha2', 'Linha3', 'Linha4', 'Linha5',
            'DiagonalPrincipal', 'DiagonalSecundaria', 'Cruzeta'
        ]
        
        # Selecionar apenas features que existem no DataFrame
        available_features = [col for col in feature_columns if col in df.columns]
        
        if available_features:
            print(f"üéØ Usando {len(available_features)} features adicionais")
            X_features = df[available_features].values
            
            # Normalizar features separadamente
            from sklearn.preprocessing import StandardScaler
            scaler_features = StandardScaler()
            X_features_norm = scaler_features.fit_transform(X_features)
            
            # Combinar n√∫meros normalizados com features
            X_numbers_norm = X_numbers / 25.0
            X_combined = np.concatenate([X_numbers_norm, X_features_norm], axis=1)
            
            print(f"üìà Matriz combinada: {X_combined.shape}")
            
        else:
            print("‚ö†Ô∏è Usando apenas n√∫meros b√°sicos (sem features avan√ßadas)")
            X_combined = X_numbers / 25.0
        
        # Para predi√ß√£o sequencial, usar o jogo anterior para prever o pr√≥ximo
        X_train = X_combined[:-1]  # Todos exceto o √∫ltimo
        y_train = X_numbers[1:] / 25.0   # Apenas n√∫meros do pr√≥ximo jogo, normalizados
        
        print(f"‚úÖ Dados preparados:")
        print(f"   - Amostras de treinamento: {X_train.shape[0]}")
        print(f"   - Features de entrada: {X_train.shape[1]}")
        print(f"   - N√∫meros de sa√≠da: {y_train.shape[1]}")
        
        return X_train, y_train, X_numbers, available_number_cols

class LoterIAModel:
    """Modelo de Deep Learning para predi√ß√£o de loteria"""
      def __init__(self, config: LoterIAConfig):
        self.config = config
        self.model = None
        
    def build_model(self, input_shape: tuple) -> tf.keras.Model:
        """Constr√≥i o modelo de rede neural OTIMIZADO"""
        print("üß† Construindo modelo de IA avan√ßado...")
        
        # Input layer
        inputs = tf.keras.layers.Input(shape=input_shape)
        
        # Primeira branch: Processamento dos n√∫meros
        numbers_branch = tf.keras.layers.Dense(128, activation='relu')(inputs)
        numbers_branch = tf.keras.layers.BatchNormalization()(numbers_branch)
        numbers_branch = tf.keras.layers.Dropout(0.3)(numbers_branch)
        
        # Segunda branch: Processamento das features estat√≠sticas
        features_branch = tf.keras.layers.Dense(64, activation='relu')(inputs)
        features_branch = tf.keras.layers.BatchNormalization()(features_branch)
        features_branch = tf.keras.layers.Dropout(0.2)(features_branch)
        
        # Concatenar branches
        combined = tf.keras.layers.Concatenate()([numbers_branch, features_branch])
        
        # Camadas densas profundas
        x = tf.keras.layers.Dense(256, activation='relu')(combined)
        x = tf.keras.layers.BatchNormalization()(x)
        x = tf.keras.layers.Dropout(0.4)(x)
        
        x = tf.keras.layers.Dense(128, activation='relu')(x)
        x = tf.keras.layers.BatchNormalization()(x)
        x = tf.keras.layers.Dropout(0.3)(x)
        
        x = tf.keras.layers.Dense(64, activation='relu')(x)
        x = tf.keras.layers.Dropout(0.2)(x)
        
        # Camada de sa√≠da com ativa√ß√£o sigm√≥ide para n√∫meros normalizados
        outputs = tf.keras.layers.Dense(15, activation='sigmoid')(x)
        
        model = tf.keras.Model(inputs=inputs, outputs=outputs)
        
        # Compilar com otimizador mais sofisticado
        optimizer = tf.keras.optimizers.Adam(
            learning_rate=0.001,
            beta_1=0.9,
            beta_2=0.999,
            epsilon=1e-07
        )
        
        model.compile(
            optimizer=optimizer,
            loss='mse',
            metrics=['mae', 'mape']  # Adicionar MAPE para melhor avalia√ß√£o
        )
        
        print("‚úÖ Modelo avan√ßado constru√≠do com sucesso")
        print(f"üìà Par√¢metros trein√°veis: {model.count_params():,}")
        
        return model
    
    def train(self, X_train, y_train, epochs=100, validation_split=0.2):
        """Treina o modelo"""
        print(f"üöÄ Iniciando treinamento ({epochs} √©pocas)...")
        
        if self.model is None:
            self.model = self.build_model(X_train.shape[1:])
        
        # Callbacks para monitoramento
        callbacks = [
            tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True),
            tf.keras.callbacks.ReduceLROnPlateau(patience=5, factor=0.5)
        ]
        
        # Treinar modelo
        history = self.model.fit(
            X_train, y_train,
            epochs=epochs,
            validation_split=validation_split,
            callbacks=callbacks,
            verbose=1
        )
        
        print("‚úÖ Treinamento conclu√≠do!")
        return history
    
    def predict(self, X_input) -> np.ndarray:
        """Faz predi√ß√µes"""
        if self.model is None:
            raise ValueError("Modelo n√£o foi treinado ainda")
        
        predictions = self.model.predict(X_input)
        
        # Desnormalizar e arredondar para n√∫meros inteiros
        predictions_denorm = (predictions * 25).round().astype(int)
        
        # Garantir que est√£o no range v√°lido
        predictions_denorm = np.clip(predictions_denorm, 1, 25)
        
        return predictions_denorm
    
    def save_model(self, filepath: str = None):
        """Salva o modelo treinado"""
        if filepath is None:
            filepath = os.path.join(self.config.MODEL_SAVE_PATH, "loteria_model.h5")
        
        if self.model:
            self.model.save(filepath)
            print(f"üíæ Modelo salvo em: {filepath}")
        else:
            print("‚ö†Ô∏è Nenhum modelo para salvar")
    
    def load_model(self, filepath: str = None):
        """Carrega modelo salvo"""
        if filepath is None:
            filepath = os.path.join(self.config.MODEL_SAVE_PATH, "loteria_model.h5")
        
        if os.path.exists(filepath):
            self.model = tf.keras.models.load_model(filepath)
            print(f"üìÇ Modelo carregado de: {filepath}")
        else:
            print(f"‚ùå Arquivo n√£o encontrado: {filepath}")

class LoterIAPredictor:
    """Sistema principal de predi√ß√£o"""
      def __init__(self):
        self.config = LoterIAConfig()
        self.config.ensure_directories()
        
        self.db_manager = DatabaseManager(self.config)
        self.data_processor = DataProcessor(self.db_manager)
        self.model = LoterIAModel(self.config)
        self.analyzer = PredictionAnalyzer(self.db_manager)  # Nova classe de an√°lise
          def run_full_pipeline(self):
        """Executa pipeline COMPLETO com an√°lises avan√ßadas"""
        print("üéØ Iniciando LoterIA - Sistema de Predi√ß√£o AVAN√áADO")
        print("=" * 60)
        
        try:
            # 1. Carregar dados hist√≥ricos
            print("\nüìä FASE 1: Carregamento de Dados")
            print("-" * 40)
            df = self.data_processor.load_historical_data()
            
            # 2. Carregar combina√ß√µes para an√°lise
            print("\nüé≤ FASE 2: Carregamento de Combina√ß√µes")
            print("-" * 40)
            combinations_df = self.analyzer.load_combinations_table(limit=100000)
            
            # 3. Preparar dados para treinamento
            print("\nüîß FASE 3: Prepara√ß√£o de Dados")
            print("-" * 40)
            X_train, y_train, X_numbers_raw, number_columns = self.data_processor.prepare_training_data(df)
            
            # 4. Treinar modelo com mais √©pocas
            print("\nüöÄ FASE 4: Treinamento do Modelo")
            print("-" * 40)
            history = self.model.train(X_train, y_train, epochs=100, validation_split=0.2)
            
            # 5. Salvar modelo
            self.model.save_model()
            
            # 6. Gerar m√∫ltiplas predi√ß√µes
            print("\nüîÆ FASE 5: Gera√ß√£o de Predi√ß√µes")
            print("-" * 40)
            
            # Gerar 5 predi√ß√µes diferentes usando entradas ligeiramente variadas
            all_predictions = []
            all_analyses = []
            
            for i in range(5):
                print(f"üéØ Gerando predi√ß√£o {i+1}/5...")
                
                # Usar √∫ltimos jogos com pequenas varia√ß√µes
                input_idx = -1 - i if len(X_train) > i else -1
                last_game = X_train[input_idx:input_idx+1]
                
                # Fazer predi√ß√£o
                prediction = self.model.predict(last_game)[0]
                
                # Garantir 15 n√∫meros √∫nicos
                unique_prediction = self._ensure_unique_numbers(prediction)
                all_predictions.append(unique_prediction)
                
                # An√°lise detalhada da predi√ß√£o
                analysis = self.analyzer.calculate_prediction_confidence(unique_prediction, df)
                all_analyses.append(analysis)
                
                print(f"   N√∫meros: {sorted(unique_prediction)}")
                print(f"   Confian√ßa: {analysis['confidence_score']:.1%}")
            
            # 7. An√°lise comparativa com combina√ß√µes
            print("\nüìà FASE 6: An√°lise Comparativa")
            print("-" * 40)
            
            best_prediction_idx = max(range(len(all_analyses)), 
                                    key=lambda i: all_analyses[i]['confidence_score'])
            best_prediction = all_predictions[best_prediction_idx]
            best_analysis = all_analyses[best_prediction_idx]
            
            print(f"üèÜ MELHOR PREDI√á√ÉO (Confian√ßa: {best_analysis['confidence_score']:.1%})")
            print(f"   N√∫meros: {sorted(best_prediction)}")
            
            # Encontrar combina√ß√µes similares
            if not combinations_df.empty:
                similar_combinations = self.analyzer.find_similar_combinations(
                    best_prediction, combinations_df, top_n=3
                )
                
                if not similar_combinations.empty:
                    print(f"\nüîç Combina√ß√µes similares encontradas:")
                    for idx, row in similar_combinations.iterrows():
                        combo_numbers = [row[f'N{i}'] for i in range(1, 16)]
                        print(f"   ID {row['ID']}: {combo_numbers}")
            
            # 8. Relat√≥rio detalhado
            print("\nüìã FASE 7: Relat√≥rio Final")
            print("-" * 40)
            self._save_detailed_predictions(all_predictions, all_analyses, best_prediction_idx)
            
            print(f"\n‚úÖ Pipeline AVAN√áADO conclu√≠do com sucesso!")
            print(f"üéØ {len(all_predictions)} predi√ß√µes geradas")
            print(f"üèÜ Melhor confian√ßa: {best_analysis['confidence_score']:.1%}")
            
        except Exception as e:
            print(f"‚ùå Erro no pipeline: {e}")
            import traceback
            traceback.print_exc()
            raise
        finally:
            self.db_manager.disconnect()
    
    def _ensure_unique_numbers(self, prediction: np.ndarray, max_attempts=10) -> np.ndarray:
        """Garante que a predi√ß√£o tenha exatamente 15 n√∫meros √∫nicos"""
        unique_nums = np.unique(prediction)
        
        if len(unique_nums) == 15:
            return unique_nums
        
        # Se temos menos de 15, preencher com n√∫meros pr√≥ximos
        if len(unique_nums) < 15:
            available = set(range(1, 26)) - set(unique_nums)
            needed = 15 - len(unique_nums)
            
            # Escolher n√∫meros que mant√™m padr√µes similares
            additional = np.random.choice(list(available), needed, replace=False)
            result = np.concatenate([unique_nums, additional])
        else:
            # Se temos mais de 15, escolher os 15 mais prov√°veis
            # (baseado na confian√ßa da rede neural)
            prob_scores = np.abs(prediction - 0.5)  # Quanto mais longe de 0.5, mais confiante
            top_indices = np.argsort(prob_scores)[-15:]
            result = prediction[top_indices]
        
        return np.sort(result).astype(int)
    
    def _save_detailed_predictions(self, all_predictions, all_analyses, best_idx):
        """Salva relat√≥rio detalhado de predi√ß√µes"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filepath = os.path.join(self.config.RESULTS_PATH, f"relatorio_detalhado_{timestamp}.txt")
        
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write("üéØ LoterIA - Relat√≥rio Detalhado de Predi√ß√µes\n")
            f.write("=" * 60 + "\n")
            f.write(f"Data/Hora: {datetime.now().strftime('%d/%m/%Y √†s %H:%M:%S')}\n")
            f.write(f"Total de predi√ß√µes: {len(all_predictions)}\n\n")
            
            # Detalhes de cada predi√ß√£o
            for i, (pred, analysis) in enumerate(zip(all_predictions, all_analyses)):
                f.write(f"PREDI√á√ÉO {i+1} {'üèÜ' if i == best_idx else ''}\n")
                f.write("-" * 30 + "\n")
                f.write(f"N√∫meros: {sorted(pred)}\n")
                f.write(f"Confian√ßa: {analysis['confidence_score']:.1%}\n")
                f.write(f"Soma total: {analysis['statistical_analysis']['soma_total']}\n")
                f.write(f"Primos: {analysis['statistical_analysis']['qtde_primos']}\n")
                f.write(f"√çmpares: {analysis['statistical_analysis']['qtde_impares']}\n")
                f.write(f"Distribui√ß√£o quintis: {analysis['statistical_analysis']['distribuicao_quintis']}\n")
                
                for rec in analysis['recommendations']:
                    f.write(f"‚Ä¢ {rec}\n")
                f.write("\n")
            
            # Resumo estat√≠stico
            f.write("RESUMO ESTAT√çSTICO\n")
            f.write("-" * 30 + "\n")
            confidences = [a['confidence_score'] for a in all_analyses]
            f.write(f"Confian√ßa m√©dia: {np.mean(confidences):.1%}\n")
            f.write(f"Melhor confian√ßa: {max(confidences):.1%}\n")
            f.write(f"Pior confian√ßa: {min(confidences):.1%}\n")
            f.write(f"\nRecomenda√ß√£o: Use a predi√ß√£o {best_idx + 1} (melhor confian√ßa)\n")
        
        print(f"üíæ Relat√≥rio detalhado salvo em: {filepath}")
        
        # Tamb√©m salvar predi√ß√£o simples para compatibilidade
        simple_filepath = os.path.join(self.config.RESULTS_PATH, f"predicao_{timestamp}.txt")
        best_prediction = all_predictions[best_idx]
        
        with open(simple_filepath, 'w', encoding='utf-8') as f:
            f.write(f"LoterIA - Predi√ß√£o gerada em {datetime.now()}\n")
            f.write("=" * 50 + "\n")
            f.write(f"N√∫meros preditos: {sorted(best_prediction)}\n")
            f.write(f"Confian√ßa: {all_analyses[best_idx]['confidence_score']:.1%}\n")
            f.write(f"Soma total: {sum(best_prediction)}\n")
        
        print(f"üíæ Predi√ß√£o principal salva em: {simple_filepath}")
    
    def _save_predictions(self, prediction):
        """Salva predi√ß√µes em arquivo"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filepath = os.path.join(self.config.RESULTS_PATH, f"predicao_{timestamp}.txt")
        
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(f"LoterIA - Predi√ß√£o gerada em {datetime.now()}\n")
            f.write("=" * 50 + "\n")
            f.write(f"N√∫meros preditos: {sorted(prediction)}\n")
            f.write(f"Soma total: {sum(prediction)}\n")
            f.write(f"N√∫meros pares: {sum(1 for x in prediction if x % 2 == 0)}\n")
            f.write(f"N√∫meros √≠mpares: {sum(1 for x in prediction if x % 2 == 1)}\n")
        
        print(f"üíæ Predi√ß√£o salva em: {filepath}")

class PredictionAnalyzer:
    """Analisador avan√ßado de predi√ß√µes usando tabela de combina√ß√µes"""
    
    def __init__(self, db_manager: DatabaseManager):
        self.db_manager = db_manager
        
    def load_combinations_table(self, limit=50000) -> pd.DataFrame:
        """Carrega combina√ß√µes da tabela COMBINACOES_LOTOFACIL"""
        print(f"üéØ Carregando {limit:,} combina√ß√µes pr√©-processadas...")
        
        try:
            query = f"""
            SELECT TOP {limit}
                ID, N1, N2, N3, N4, N5, N6, N7, N8, N9, N10,
                N11, N12, N13, N14, N15,
                QtdePrimos, QtdeFibonacci, QtdeImpares, SomaTotal,
                Quintil1, Quintil2, Quintil3, Quintil4, Quintil5,
                QtdeGaps, QtdeRepetidos, SEQ, DistanciaExtremos,
                ParesSequencia, QtdeMultiplos3, ParesSaltados,
                Faixa_Baixa, Faixa_Media, Faixa_Alta
            FROM COMBINACOES_LOTOFACIL
            ORDER BY NEWID()  -- Ordem aleat√≥ria para variedade
            """
            
            if self.db_manager.config.DB_TYPE.lower() == "sqlserver":
                import urllib.parse
                
                params = urllib.parse.quote_plus(
                    f"DRIVER={{{self.db_manager.config.SQL_DRIVER}}};"
                    f"SERVER={self.db_manager.config.SQL_SERVER};"
                    f"DATABASE={self.db_manager.config.SQL_DATABASE};"
                    f"Trusted_Connection=yes;"
                )
                engine = create_engine(f"mssql+pyodbc:///?odbc_connect={params}")
                
                df = pd.read_sql(query, engine)
                print(f"‚úÖ {len(df):,} combina√ß√µes carregadas")
                return df
            else:
                print("‚ö†Ô∏è Tabela de combina√ß√µes dispon√≠vel apenas no SQL Server")
                return pd.DataFrame()
                
        except Exception as e:
            print(f"‚ùå Erro ao carregar combina√ß√µes: {e}")
            return pd.DataFrame()
    
    def calculate_prediction_confidence(self, prediction: np.ndarray, historical_data: pd.DataFrame) -> dict:
        """Calcula confian√ßa da predi√ß√£o baseada em padr√µes hist√≥ricos"""
        print("üìä Analisando confian√ßa da predi√ß√£o...")
        
        analysis = {
            'prediction': prediction.tolist(),
            'confidence_score': 0.0,
            'pattern_matches': [],
            'statistical_analysis': {},
            'recommendations': []
        }
        
        # An√°lise estat√≠stica b√°sica
        pred_sum = int(np.sum(prediction))
        pred_primes = sum(1 for n in prediction if n in [2,3,5,7,11,13,17,19,23])
        pred_odds = sum(1 for n in prediction if n % 2 == 1)
        
        # Comparar com hist√≥rico
        if len(historical_data) > 0:
            hist_sums = historical_data['SomaTotal'].values if 'SomaTotal' in historical_data.columns else []
            hist_primes = historical_data['QtdePrimos'].values if 'QtdePrimos' in historical_data.columns else []
            hist_odds = historical_data['QtdeImpares'].values if 'QtdeImpares' in historical_data.columns else []
            
            # Calcular desvios
            if len(hist_sums) > 0:
                sum_percentile = np.percentile(hist_sums, 50)
                sum_deviation = abs(pred_sum - sum_percentile) / sum_percentile
                analysis['statistical_analysis']['sum_deviation'] = sum_deviation
                
                # Confian√ßa baseada na proximidade com a mediana hist√≥rica
                confidence_sum = max(0, 1 - sum_deviation)
                analysis['confidence_score'] += confidence_sum * 0.4
            
            if len(hist_primes) > 0:
                primes_mode = np.bincount(hist_primes).argmax() if len(hist_primes) > 0 else 0
                primes_match = 1 if pred_primes == primes_mode else 0.5
                analysis['confidence_score'] += primes_match * 0.3
            
            if len(hist_odds) > 0:
                odds_mode = np.bincount(hist_odds).argmax() if len(hist_odds) > 0 else 0
                odds_match = 1 if pred_odds == odds_mode else 0.5
                analysis['confidence_score'] += odds_match * 0.3
        
        # An√°lise de distribui√ß√£o por quintis
        quintil_dist = [
            sum(1 for n in prediction if 1 <= n <= 5),   # Quintil 1
            sum(1 for n in prediction if 6 <= n <= 10),  # Quintil 2
            sum(1 for n in prediction if 11 <= n <= 15), # Quintil 3
            sum(1 for n in prediction if 16 <= n <= 20), # Quintil 4
            sum(1 for n in prediction if 21 <= n <= 25)  # Quintil 5
        ]
        
        analysis['statistical_analysis'] = {
            'soma_total': pred_sum,
            'qtde_primos': pred_primes,
            'qtde_impares': pred_odds,
            'qtde_pares': 15 - pred_odds,
            'distribuicao_quintis': quintil_dist,
            'amplitude': int(max(prediction) - min(prediction)),
            'sequencias': self._count_sequences(prediction)
        }
        
        # Recomenda√ß√µes baseadas na an√°lise
        if analysis['confidence_score'] > 0.7:
            analysis['recommendations'].append("üü¢ Alta confian√ßa - Padr√£o bem alinhado com hist√≥rico")
        elif analysis['confidence_score'] > 0.5:
            analysis['recommendations'].append("üü° Confian√ßa m√©dia - Alguns padr√µes divergem do hist√≥rico")
        else:
            analysis['recommendations'].append("üî¥ Baixa confian√ßa - Padr√£o muito divergente do hist√≥rico")
            
        return analysis
    
    def _count_sequences(self, numbers: np.ndarray) -> int:
        """Conta sequ√™ncias consecutivas nos n√∫meros"""
        sorted_nums = sorted(numbers)
        sequences = 0
        current_seq = 1
        
        for i in range(1, len(sorted_nums)):
            if sorted_nums[i] == sorted_nums[i-1] + 1:
                current_seq += 1
            else:
                if current_seq >= 2:
                    sequences += 1
                current_seq = 1
                
        if current_seq >= 2:
            sequences += 1
            
        return sequences
    
    def find_similar_combinations(self, prediction: np.ndarray, combinations_df: pd.DataFrame, top_n=5) -> pd.DataFrame:
        """Encontra combina√ß√µes similares na tabela de combina√ß√µes"""
        print(f"üîç Buscando {top_n} combina√ß√µes mais similares...")
        
        if combinations_df.empty:
            return pd.DataFrame()
        
        # Calcular similaridade baseada nas features estat√≠sticas
        pred_features = self._extract_features(prediction)
        
        similarities = []
        for idx, row in combinations_df.iterrows():
            comb_features = {
                'QtdePrimos': row['QtdePrimos'],
                'QtdeFibonacci': row['QtdeFibonacci'],
                'QtdeImpares': row['QtdeImpares'],
                'SomaTotal': row['SomaTotal'],
                'Quintil1': row['Quintil1'],
                'Quintil2': row['Quintil2'],
                'Quintil3': row['Quintil3'],
                'Quintil4': row['Quintil4'],
                'Quintil5': row['Quintil5']
            }
            
            # Calcular dist√¢ncia euclidiana normalizada
            distance = 0
            for feature in pred_features:
                if feature in comb_features:
                    # Normalizar para features diferentes
                    if feature == 'SomaTotal':
                        weight = 1.0 / 100  # Normalizar soma
                    else:
                        weight = 1.0
                    distance += ((pred_features[feature] - comb_features[feature]) * weight) ** 2
            
            similarity = 1 / (1 + np.sqrt(distance))
            similarities.append((idx, similarity))
        
        # Ordenar por similaridade e pegar top_n
        similarities.sort(key=lambda x: x[1], reverse=True)
        top_indices = [x[0] for x in similarities[:top_n]]
        
        return combinations_df.iloc[top_indices].copy()
    
    def _extract_features(self, numbers: np.ndarray) -> dict:
        """Extrai features estat√≠sticas de um conjunto de n√∫meros"""
        return {
            'QtdePrimos': sum(1 for n in numbers if n in [2,3,5,7,11,13,17,19,23]),
            'QtdeFibonacci': sum(1 for n in numbers if n in [1,2,3,5,8,13,21]),
            'QtdeImpares': sum(1 for n in numbers if n % 2 == 1),
            'SomaTotal': int(np.sum(numbers)),
            'Quintil1': sum(1 for n in numbers if 1 <= n <= 5),
            'Quintil2': sum(1 for n in numbers if 6 <= n <= 10),
            'Quintil3': sum(1 for n in numbers if 11 <= n <= 15),
            'Quintil4': sum(1 for n in numbers if 16 <= n <= 20),
            'Quintil5': sum(1 for n in numbers if 21 <= n <= 25)
        }
def main():
    """Fun√ß√£o principal"""
    print("üöÄ LoterIA - Sistema de Predi√ß√£o de Loteria")
    print("Vers√£o 1.0.0 - Desenvolvido com TensorFlow")
    print()
    
    try:
        # Criar inst√¢ncia do preditor
        predictor = LoterIAPredictor()
        
        # Executar pipeline completo
        predictor.run_full_pipeline()
        
    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è Execu√ß√£o interrompida pelo usu√°rio")
    except Exception as e:
        print(f"\n‚ùå Erro cr√≠tico: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
